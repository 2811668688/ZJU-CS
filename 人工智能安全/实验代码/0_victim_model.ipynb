{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12402be5",
   "metadata": {},
   "source": [
    "# Load Cora Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2998ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cpu')\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data = torch.load('data.pth')\n",
    "g = data['g'].to(device)\n",
    "feat = data['feat'].to(device)\n",
    "label = data['label'].to(device)\n",
    "train_nodes = data['train_nodes']\n",
    "val_nodes = data['val_nodes']\n",
    "test_nodes = data['test_nodes']\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df9d22",
   "metadata": {},
   "source": [
    "# Define GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb9624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"Graph Convolution Network (GCN)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    # GCN with one hidden layer\n",
    "    >>> model = GCN(100, 10, hid=32)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 hid: list = 16,\n",
    "                 dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hid)\n",
    "        self.conv2 = GraphConv(hid, out_feats)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "\n",
    "        if torch.is_tensor(g):\n",
    "            feat = self.dropout(feat)\n",
    "            feat = g @ (feat @ self.conv1.weight) + self.conv1.bias\n",
    "            feat = F.relu(feat)\n",
    "            feat = self.dropout(feat)\n",
    "            feat = g @ (feat @ self.conv2.weight) + self.conv2.bias\n",
    "            return feat\n",
    "        \n",
    "        g = g.add_self_loop()\n",
    "        feat = self.dropout(feat)\n",
    "        feat = self.conv1(g, feat)\n",
    "        feat = F.relu(feat)\n",
    "        feat = self.dropout(feat)\n",
    "        feat = self.conv2(g, feat)\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634a99c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9932b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.5081, Val: 0.4498, Test: 0.4386\n",
      "Epoch: 002, Train: 0.5806, Val: 0.5382, Test: 0.5045\n",
      "Epoch: 003, Train: 0.6492, Val: 0.5502, Test: 0.5337\n",
      "Epoch: 004, Train: 0.6815, Val: 0.5703, Test: 0.5578\n",
      "Epoch: 005, Train: 0.6935, Val: 0.5904, Test: 0.5795\n",
      "Epoch: 006, Train: 0.7177, Val: 0.6185, Test: 0.6021\n",
      "Epoch: 007, Train: 0.7460, Val: 0.6426, Test: 0.6408\n",
      "Epoch: 008, Train: 0.7823, Val: 0.7108, Test: 0.6781\n",
      "Epoch: 009, Train: 0.8145, Val: 0.7430, Test: 0.7108\n",
      "Epoch: 010, Train: 0.8347, Val: 0.7631, Test: 0.7309\n",
      "Epoch: 011, Train: 0.8468, Val: 0.7671, Test: 0.7445\n",
      "Epoch: 012, Train: 0.8548, Val: 0.7751, Test: 0.7545\n",
      "Epoch: 013, Train: 0.8629, Val: 0.7751, Test: 0.7545\n",
      "Epoch: 014, Train: 0.8790, Val: 0.7831, Test: 0.7676\n",
      "Epoch: 015, Train: 0.8831, Val: 0.7831, Test: 0.7676\n",
      "Epoch: 016, Train: 0.8911, Val: 0.7992, Test: 0.7817\n",
      "Epoch: 017, Train: 0.9234, Val: 0.8233, Test: 0.8048\n",
      "Epoch: 018, Train: 0.9274, Val: 0.8434, Test: 0.8154\n",
      "Epoch: 019, Train: 0.9355, Val: 0.8594, Test: 0.8280\n",
      "Epoch: 020, Train: 0.9476, Val: 0.8675, Test: 0.8385\n",
      "Epoch: 021, Train: 0.9516, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 022, Train: 0.9556, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 023, Train: 0.9597, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 024, Train: 0.9677, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 025, Train: 0.9718, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 026, Train: 0.9758, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 027, Train: 0.9839, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 028, Train: 0.9879, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 029, Train: 0.9879, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 030, Train: 0.9879, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 031, Train: 0.9919, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 032, Train: 0.9919, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 033, Train: 0.9919, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 034, Train: 0.9960, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 035, Train: 0.9960, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 036, Train: 0.9960, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 037, Train: 0.9960, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 038, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 039, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 040, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 041, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 042, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 043, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 044, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 045, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 046, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 047, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 048, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 049, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 050, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 051, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 052, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 053, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 054, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 055, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 056, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 057, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 058, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 059, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 060, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 061, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 062, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 063, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 064, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 065, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 066, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 067, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 068, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 069, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 070, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 071, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 072, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 073, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 074, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 075, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 076, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 077, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 078, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 079, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 080, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 081, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 082, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 083, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 084, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 085, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 086, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 087, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 088, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 089, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 090, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 091, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 092, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 093, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 094, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 095, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 096, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 097, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 098, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 099, Train: 1.0000, Val: 0.8755, Test: 0.8436\n",
      "Epoch: 100, Train: 1.0000, Val: 0.8755, Test: 0.8436\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(model(g, feat)[train_nodes], label[train_nodes]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(g, feat), []\n",
    "    for nodes in (train_nodes, val_nodes, test_nodes):\n",
    "        pred = logits[nodes].max(1)[1]\n",
    "        acc = pred.eq(label[nodes]).float().mean()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "num_feats = feat.size(1)\n",
    "num_classes = int(label.max() + 1)\n",
    "model = GCN(num_feats, num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=0.01)  # Only perform weight-decay on first convolution.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4056d2",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1da1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target label:  tensor(2)\n",
      "model predict:  tensor(2)\n"
     ]
    }
   ],
   "source": [
    "target = 1\n",
    "target_label = label[target]\n",
    "print(\"target label: \", target_label)\n",
    "\n",
    "print(\"model predict: \", model(g, feat)[target].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e81ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vitim model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
