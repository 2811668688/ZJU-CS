# 人工智能安全

[TOC]

并不全，同还是阳了两周，然后就有点躺了。

后面讲的伦理和可解释性，也都非常有意思（不过后期的课程我在别的课里学过，所以没有记）

## 导入

#### 1. 成绩构成

- 无考试
- 出勤：10%，不点名，但是课上会提问，一次扣2分
- 平时作业：30%，思考题和报告

- 上机：20%，写上机的心得报告
- 期末大作业：40%，写报告

#### 2. 课程安排

- 教师理论+蚂蚁专家
- 春学期不上实验课，夏学期上实验课，8周上满，4节讲座4个实验上机

- 参考书籍：《AI安全之对抗样本入门》

## 概论

#### 1. 引入

- 人工智能，牵扯到视觉，图像音频识别，决策等。

- 图灵测试：超过30%的测试者无法分别AI和人类，则认为通过了图灵测试。例如chatgpt

- 学科关系：AI>机器学习>表示学习>深度学习（狭义上，通过多层神经网络）
- 本课程，关注的是**深度学习**上的**安全性能**

#### 2. AI发展史

- 朴素、形式化、规则化、列表化的环境中完成任务。eg.1997年IBM深蓝国际象棋领域夺冠。

- 知识库（knowledge base）方法，将海量知识用计算机可存储识别的方式进行 **硬编码（hard-code）**，使用逻辑推理来理解世界。eg.Cyc，但是逻辑推延能力很低，不具备从原始数据上提取模式的能力。

  *补充：过拟合和欠拟合，通俗上也可以从训练集大小和训练程度来进行类比，例如图像识别上，喂的东西太少会很难识别，但是学习的过多，又使得明显是狗的图片被不能识别*

- 进入机器学习

#### 3. 机器学习建模

- 明确问题：本质是分类（不具可比性，例如动物、天气识别，**离散**）和回归（预测天气，房价，可比较，**连续**）问题

- 明确数据选择：数据具有代表性，无代表性则 **拟合性差**；数据时间范围，不能因果倒置；数据业务范围：不能引入噪音

- 特征工程，对原始数据进行数据分析,eg.进行数据合成来弥补不存在的数据（帽子+男人=戴帽子的男人）；对数据预处理，eg.处理异常值；对特征提取.

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230228171021600.png" alt="image-20230228171021600" style="zoom:70%;" />

- 模型训练，**training set+validation set+testing set**，验证集和测试集的区别是：验证集是参与训练的，可以判断精确度来调整，测试集不参与训练。
- 模型评估：两个词语，欠拟合，eg.只有一层神经网络,无法学习；过拟合，eg.模型无需总结规律

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230228171636177.png" alt="image-20230228171636177" style="zoom:50%;" />

#### 4. 表示学习

- 例子：更换坐标系（不同的表示下），可以轻易完成划分点的任务。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230228172205892.png" alt="image-20230228172205892" style="zoom:50%;" />

- 定义：模型自动从数据中抽取特征或表示的方法，典型的是自编码器

  <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230228172446249.png" alt="image-20230228172446249" style="zoom:80%;" />

#### 5. 深度学习

- 小任务：刻画边缘的方法是，比较周边像素
- 例子：每一层完成特定层的任务，逐层合并起来实现目标功能。
- 经典例子：MLP，多层感知机



## 概论续

#### 1. 人工智能安全问题

- 概览：数据污染（影响模型训练）、算法漏洞（导致决策失误）、隐私侵犯、种族歧视（产生AI公平问题）、对抗样本攻击（对样本添加微小扰动，导致AI错判，例如使得摄像头无法对焦识别）
- **分类一：组成要素：根据不同的模型与数据**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230307162525882.png" alt="image-20230307162525882" style="zoom:60%;" />

​		**数据安全威胁**：从低维推测高维，侵犯数据隐私（例如从图像识别最后得到的向量推断原图片）

​		**模型安全威胁**：图像识别中加噪声，使得不能正确判断类别（例如条纹口罩，使得权限成为最大），并且这种威胁没有好的防御手段

​		**运行环境威胁**：各种运行中进行影响

- **分类二：依据生命周期**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230307163924051.png" alt="image-20230307163924051" style="zoom:60%;" />

​		**设计阶段威胁：**未能均匀采样，在标签上产生偏见，种族上产生歧视

​		**模型训练阶段：**数据投毒指使系统不能正常运作，后门攻击指设计触发器，模型预测结果会被恶行篡改

​		**执行阶段威胁：**设计特定的扰动，使得test集直接报错/推断一个数据是不是训练集，以此来得到隐秘信息

- **分类三：系统架构分类**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230307165101843.png" alt="image-20230307165101843" style="zoom:60%;" />

#### 2. 特定词汇解释

- 在机器学习中，鲁棒性通常是指模型对于输入数据中的干扰、扰动或攻击具有一定程度的容忍度

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230307170937737.png" alt="image-20230307170937737" style="zoom:60%;" />

- 深度伪造：利用深度学习进行模拟，然后伪造一些事物出来进行欺诈。





## 神经网络

#### 1. 引入：数字识别案例

​	数字识别：将一张图像转化为1与0的序列。若每个数字定义一个模式（由1与-1形成的特定矩阵），那么可以定义点积net，以此来通过结果，判断是否为模式对应的数字

![image-20230314162239999](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314162239999.png)

​	如正确是143，**不对应的数字net会偏大/偏小**	![image-20230314162421835](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314162421835.png)

​	但是**数字笔画大小**会影响结果



#### 2. 激活函数与神经元

​	使用sigmod函数，将匹配结果拉到0-1之间，老师讲法为：这个**很像神经元中的突触**，所以在深度学习中被使用。在 李宏毅老师的课中，这一块是用：“sigmod函数**便于叠加**起来，**生成一个较为复杂的函数**对应关系”来引入的（当然也可以使用relu）。当然，这里的叠加和神经网络中的神经元传递是很像的

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314162823075.png" alt="image-20230314162823075" style="zoom:50%;" />

​	由于在上述案例中，大部分数字都很大， 使得结果偏1，无法比较，因此使用了 **偏置项**，即f(x+b)中的b ，这样可以让函数的结果对x有区分度。而根据 **结果大小**，可以定义这个神经元是否被激活了。

​	![image-20230314163333034](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314163333034.png)

​	用本图做解释， **激活与否**就是一个函数得到的大小，再当作 **xi** 传递到下一个神经元

​	又 **计算方便**：为了将偏置项简化表达形式，当成1.x0放入表达式



#### 3. 其他激活函数

例如：sgn函数

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314163711247.png" alt="image-20230314163711247" style="zoom:50%;" />

例如：线性整流函数（常用到的）

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314163816886.png" alt="image-20230314163816886" style="zoom:50%;" />

例如：Sofrmax（分类问题中常用，和为1）

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314163930129.png" alt="image-20230314163930129" style="zoom:50%;" />

#### 4. 神经网络

- 举例：fully connected network：每一个神经元都会被下一层的神经元所使用到

- 训练：

  - 建立数据集（数据标注label，training set，validation set和testing set，数据翻转，预训练处理后制造数据，修改数据，加噪等）

  - loss function、optimization、learning rate。例子中的1/2是为了某种稳定性；而optimization又有很多优化的手段，基础中的梯度下降法，沿着梯度下降的方向走步长，并且发现小批量下降还会更好

    <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314165505853.png" alt="image-20230314165505853" style="zoom:50%;" />

    <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230314170758003.png" alt="image-20230314170758003" style="zoom:50%;" />

  - 反向传播：从和结果最近的那一批参数开始更新，然后逐步用更新得到的结果一步步往前更新

  - crossentropy函数，用于分类问题，误差平方和是用于求值的问题

- 优化：Jacobian矩阵（一阶导）和Hessian矩阵（二阶导）

  



## 对抗样本攻击

#### 1. 人工智能模型特性

- 网络深，结构复杂

- 可解释性不足

- 高维空间（高维向量）

- 理论研究不足

#### 2. 对抗样本攻击原理

- 举例：在stop图上加一些小标志，使识别结果错误

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321162512525.png" alt="image-20230321162512525" style="zoom:50%;" />

- 分类：

  白盒指知道模型的参数、结构；黑盒只能知道输入和输出的结果

  有目标攻击指，想让模型一遇到攻击就发送特定动作（让熊猫必须识别为猫），无目标攻击只要错误，不指定类别；

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321162727802.png" alt="image-20230321162727802" style="zoom:50%;" />

- 逃逸攻击：

  让分类器无法分辨这是坏的，逃过分类

- 衡量标准：范数

  - L0范数：表示向量中非零元素的个数；
  - L1范数：表示向量中各个元素的绝对值之和，即Manhattan距离；
  - L2范数：表示向量各元素的平方和的平方根，即Euclidean距离；
  - Linf范数：表示向量中各元素绝对值的最大值。

- 方法一：使代价函数最小

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321164123098.png" alt="image-20230321164123098" style="zoom:50%;" />

- 方法二：允许一定的代价变换，但是使得越违背训练目标越好，例如：**让loss越大越好**，也就是尽可能**不被分类成希望的label**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321164409463.png" alt="image-20230321164409463" style="zoom:50%;" />

​							或者还可以引入概率



<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321164558879.png" alt="image-20230321164558879" style="zoom:43%;" />

- 方法三：同时最小化 **目标函数** 和 **代价函数**

  ​	

  <img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230321164817786.png" alt="image-20230321164817786" style="zoom:50%;" />





#### 3. 对抗训练防御

- 主动生产对抗样本，同样也纳入训练，增强鲁棒性
- 可以手动生成有标签的
- 可以生成无标签的，样本从原数据变化，标签和原样本的标签要一致
- PGD对抗训练：虽然不用正常样本，全用对抗样本，但是对一个正常样本用很多微小扰动后的对抗样本做替换，就像一个小圆被一堆包围，所以效果也很好

​	



## 人工智能完整性之数据投毒

#### 1. 数据投毒

**例子：** 伪造数据集，使得模型失去广泛匹配项，比如买羽毛球和球拍联系在一起，攻击方制造众多羽毛球和小鸭子一起联系的数据。污染训练集，这样子可以给小鸭子免费做广告，模型一看到有人买球，就会推荐小鸭子。

**基本概念：** 数据清洗为什么不能把染毒数据给去掉呢？ 一般认为，收集数据和数据预处理是攻击者可以进行破坏的。

比如：将中毒样本投入到网络中，让训练者收集时把这些样本纳入训练集

比如：预处理时，内部人员特别插入，修改一部分数据，进行投毒

**目的：**无目标和有目标，只要错就好和特定引导

**知识分类**：白盒（全知），黑盒（通过收集替代数据集，自己构建一个替代模型来做模拟），灰盒（部分了解）

**攻击能力**：标签（label）修改；数据扰动（对部分数据添加噪声等）；数据注入（制造大量虚假用户去注入虚假内容）

**建模：**（s.t. subject to） 一个是想让染毒样本x在validation集上的损失是最大的

​											一个是想让染毒后，模型是能稳定的



#### 2. 攻击方法

##### 翻转标签法

随机反转一些样本的标签来达到毒化效果，但是因为要有隐蔽性，可能会被数据清洗给除掉

##### 梯度下降法

改投毒数据的时候往梯度的相反方向跑；如果要控制特定的样本，可以通过加掩码等人为的进行控制；如果想对染毒做约束，可以再加一个st，然后对x'-x做一个范数约束

##### 干净标签法

特征碰撞法，我们有船的图片，制造很多样本，在**像素上还是船**，**标签上也是船**，但是**某层特征空间（模型用的判断依据）**里却很像青蛙。

注意：这是干净标签，所以是从第k层开始像青蛙，不能修改最后一层，不然直接被分成青蛙了

模型被添加了这样的样本后，就会改变自己，把特征空间里是青蛙特征的样本，认为其标签应该是一条船。

**要污染很多样本**，不然鲁棒性强的不会因为一个染毒样本就改变自身的判断





#### 3. 防御方法

被动和主动，一个基于数据检测的防御，一个是基于鲁棒性训练和基于数据增强的防御

##### 基于数据检测防御

**KNN**：找特征空间里的邻居，和比重最大的邻居属性保持一致。比如防御干净标签，在特征空间里发现，一张图周围一圈都是青蛙，但是你特征在青蛙圈里，标签却是船，就可能是染毒样本。 **可能要多层过滤**，只要到最后染毒样本很少，一般最后就不会有问题，不会影响到模型



##### 提高鲁棒性

使得训练集即使有一些染毒样本，依然保持正确率

基于集成学习的防御：

- 对训练集做划分，在不同的子集上训练各自的模型
- 根据自己和投票结果产生最终预测结果
- 分散了投毒样本对模型的影响
- 要求训练集要足够大，不然错误样本过多直接弄成错误的



##### 数据增强

- 基于mixup和CutMix的主动防御方法
- mixup：通过两个点连一条线，在线上随机取点，插入到数据之中。而标签的确定上，maybe也可以引入软标签，从0和1变成0.8这样的，和这个点到已知的两点的距离有关
- cutmix：将原始图片的一部分切割掉，在其中随机填充训练集的其他数据区域像素，然后软化标签（非0和1）



## 后门攻击

在特定条件下触发，比如规定戴墨镜的人，都会识别成川普

#### 1. 方法

​	在训练阶段，攻击者借助数据投毒的方法，污染训练集/直接修改模型

- 数据投毒的办法：使用带有除法器的数据将隐藏的后门嵌入深度神经网络
- 触发器名词：用于激活隐藏后门的图案
- 源类别：中毒图像原本的类别
- 目标类别：希望攻击成的结果类别
- 攻击成功率Attack success Rate：被毒化后错误分类的个数/毒化的总个数
- 良性样本准确率Benign accuracy ：毒化模型在良性测试样本上的预测准确率
- xc=x+trigger
- 后门攻击都是有目标攻击
- 模型外包：只拿到了模型M和参数w，拿不到数据



**基于数据投毒的BadNets后门攻击**：可以训练模型，自己特别设计一个触发器

**基于模型投毒的Trojan后门攻击：**在网络的倒数第二层（softmax前面的那个），找脆弱神经元（就是对触发器敏感，激活值最大的一个或者几个神经元），使得其激活值更大（给trigger添加改变像素等），更脆弱，以至于可以影响输出；然后生成B类别的高置信度的数据均分D1D2；一半标签改变，一半保持不变；微调参数，重新调一下模型



#### 2. 防御

##### 检测

白盒模型后门检测：

- 优点：

  - 可深入：能够深入分析模型的内部结构和算法，从而可以检测到一些深度隐藏的后门

  - 准确性高：白盒模型后门检测可以查看模型内部的结构和参数，对于被植入后门的模型特征进行全面的分析和检测，下面的图里就列举了课上讲过的检测受损的神经元的思想

  - 适用范围广：白盒模型后门检测不需要对攻击方式进行假设或猜测，能够较好地适应各种类型的后门攻击

    ![image-20230515235052685](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230515235052685.png)

- 缺点：

  - 实现困难：需要获取模型的源代码和参数信息，这在实践中难以实现。有些模型检测如果不是内部检测，可能无法给检测方
  - 耗费资源较多：为了进行深度的白盒后门检测，需要监控模型的所有输入和输出以及大模型，这可能会需要大量运行时间和存储空间
  - 对模型产生修改：为了弥补问题，可能会模型参数进行修改，这之中会影响到模型的准确度

​	黑盒模型后门检测:

- 优点

  - 易于开展：不需要获取模型的源代码和参数信息，只需要利用输入和输出进行检测，因此比较容易实现。
  - 不会影响模型：不会对模型进行修改，可以确保不影响到大部分样本的准确度，可以保证模型不会因为检测而产生较大变化

- 缺点

  - 检测难以深入：无法分析模型的内部结构和算法，无法检测到一些深层次的后门。

  - 准确性较低：黑盒后门检测只能根据模型的输入和输出进行分析，很难发现一些深度隐藏的后门、或者需要多步操作才能触发的后门。但这并不绝对，比如课堂上讲过的基于元学习的方法，就甚至超过了白盒检测

    ![image-20230515235249592](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230515235249592.png)

  白盒模型后门检测适用于：

- 模型的安全需求较高的场景；

- 可以获得模型源代码和参数信息的情况下，最好是内部人员自己检测；

- 需要进行深度检查，确保所有可能的后门都被发现的场景；

- 需要对已知攻击类型进行检测的场景。

  

  黑盒模型后门检测适用于：

- 无法获得模型源代码和参数信息的情况下；

- 需要对多个模型进行**快速**检测的场景；

- 可以使用各种类型的数据集进行测试的场景；

- 需要对未知攻击类型进行检测的场景。



##### 防范

- 安全开发规范：**训练之前**，使用安全编码标准、漏洞扫描工具和安全测试工具，建立安全开发流程和制定相应的安全政策和规范，确保整个开发过程的安全性和可追溯性，以减小有人破坏模型的可能性动机。

- 训练数据检测：**训练之前**，还可以使用数据清洗和预处理技术来减少后门注入的可能性。例如，可以使用去噪和去重技术来减少冗余和无效数据，可以使用数据聚类和分类算法来判断数据是否符合正常分布，并及时删除异常数据。
- 训练数据增强：**训练之中，**可以设计一些特定的攻击数据混入其中，来提高模型的鲁棒性，不至于很容易被攻击
- 限制模型访问权限：**训练之中**，使用身份认证和授权技术，并在模型部署时设置多层次的访问权限，例如，根据用户的角色和职责设置不同的访问级别和权限。此外，还可以为模型建立操作审计日志，并定期对日志进行分析和审计。以断绝训练时有人恶意注入后门
- 设置安全密钥：**训练之中**，为了确保模型在训练和部署过程中不会被篡改或污染，可以采取一些安全措施来加强模型的安全性，如加密、签名、验证等。例如，可以使用加密技术对模型进行加密，只有有权访问密钥的人才可以进行解密。此外，可以使用数字签名对模型进行签名，以确保没有人对模型进行了更改。
- 模型监控：**训练之中**，使用异常检测技术和事务管理器，使用流量分析和行为分析技术来监控模型的输入和输出，及时发现和处理异常情况。此外，还可以使用机器学习技术来自动化模型的监控和管理，从而减轻人工干预的压力。
- 多模型检测：**训练之中**，还可以使用模型退化技术（model degradation）来掩盖模型的关键信息，从而降低模型被攻击的风险。

- 模型评估：**训练之中**，我认为还可以在某些迭代次数下，就停下来及时进行一批检测，以及时发现是否会有风险





## 安全隐私性

#### 1. 案例

- 数据获取、数据撞库、数据销售、数据犯罪；下面引入一个撞库的定义，并且，撞库的过程中最主要的障碍就是各个网站设置的验证码

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522184551439.png" alt="image-20230522184551439" style="zoom:50%;" />



​		攻击流程为：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522184833912.png" alt="image-20230522184833912" style="zoom:30%;" />



- 隐私保护目前没有标准的形式化定义，被主流接受的是通过差分隐私定义的隐私保护（Differential Privacy）

#### 2. 差分隐私

​	**差分攻击：**

​	假设现在有一个婚恋数据库，2个单身97个已婚，只能查有多少人单身。攻击者第一次查询发现，2个人单身；张三登记了自己婚姻状况后，攻击者再次查询发现三个单身，所以张三单身。张三作为一个样本的出现，使得攻击者获得了新的知识。

​	**差分隐私：**

​	实现原理：差分隐私要求我们所发布的原始数据要**经过一个随机算法**来处理，随机算法会对原始数据**做一定程度的扰动**，即加入随机噪声，来使这个攻击者**无法利用扰动后的信息反推**某个人是否存在于原数据里面。

​	上面的例子中，本来两次查询结果是确定的2和3，加入随机噪声后，变成了两个随机变量，画出概率密度函数图，两次可能都返回2.5

​	**保护实例：**

![image-20230522185612870](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522185612870.png)



#### 3. 人工智能隐私

​    数据隐私：攻击者不能从模型的输出推测出输入数据、训练数据集的有用信息

- 成员推断攻击（MembershipInferenceAttack）

- 模型逆向（反演）攻击（ModelInversionAttack）

​	模型隐私：模型的参数、结构不被攻击者掌握、获取

- 模型窃取攻击（ModelExtractionAttacks）

![image-20230522190235730](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522190235730.png)

#### 4. 隐私保护算法

**对数据隐私的保护**

- 预测阶段的数据隐私保护算法：在模型的预测阶段施加防御手段

- 训练阶段的数据隐私保护算法：在模型的训练阶段施加防御手段

**对模型版权的保护**

- 数字水印（DigitalWatermark）

![image-20230522190259600](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522190259600.png)

![image-20230522190312605](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20230522190312605.png)

- 基于训练阶段的隐私方案如DP-SGD和数字水印的时间成本体现在模型训练的阶段，通过修改模型的损失函数和梯度来添加防御措施

- 基于训练阶段的隐私方案对于用户来说在时间上的区分并不大，但是由于在训练阶段增加了隐私防御的目标，通常会导致模型收敛慢，训练准确率下降等的问题

- 基于预测阶段的隐私方案如MemGuard的时间成本体现在模型预测的阶段，通过修改模型的置信度向量来添加防御措施

- MemGuard对于用户的影响较大，每次需要较长的时间来寻找置信度向量的对抗样本